# ğŸ“ RAG Q&A With PDF + Chat History

This project is a **Conversational RAG (Retrieval-Augmented Generation)** app built with **Streamlit**.  
It allows you to **upload PDFs** and chat with their content while maintaining **chat history**.

---

## ğŸš€ Features
- ğŸ“‚ Upload multiple PDF files.
- ğŸ” Retrieve context-aware answers using **LangChain + Chroma Vector DB**.
- ğŸ¤– Uses **Groq LLM (Gemma2-9b-It)** for natural conversation.
- ğŸ§  Maintains **chat history across sessions**.
- âœ‚ï¸ Text is split & embedded using **HuggingFace embeddings**.

---

## ğŸ› ï¸ Tech Stack
- [Streamlit](https://streamlit.io/) â€“ UI framework  
- [LangChain](https://www.langchain.com/) â€“ Conversational AI framework  
- [Chroma](https://www.trychroma.com/) â€“ Vector database  
- [HuggingFace Embeddings](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)  
- [Groq API](https://groq.com/) â€“ LLM inference  

---

## ğŸ“‚ Project Structure
5.RAG_QA_Conversation/
â”‚â”€â”€ app.py # Main Streamlit application
â”‚â”€â”€ temp.pdf # Sample PDF for testing
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md # Project documentation


---

## âš™ï¸ Setup Instructions

1ï¸âƒ£ Clone the repo:
```bash
git clone https://github.com/your-username/rag-qa-pdf.git
cd rag-qa-pdf
2ï¸âƒ£ Create & activate virtual environment:

bash
Copy code
python -m venv venv
source venv/bin/activate    # Mac/Linux
venv\Scripts\activate       # Windows
3ï¸âƒ£ Install dependencies:

bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Create a .env file:

env
Copy code
HF_TOKEN=your_huggingface_token
5ï¸âƒ£ Run the app:

streamlit run app.py
```
## ğŸ”‘ API Keys Required
Groq API Key â†’ Enter inside the Streamlit UI when prompted.

HuggingFace Token â†’ Stored in .env file.

## ğŸ“¦ Requirements
Here are the dependencies used in this project:

streamlit
langchain
langchain-community
langchain-core
langchain-chroma
langchain-groq
langchain-huggingface
langchain-text-splitters
chromadb
huggingface-hub
python-dotenv
pypdf
 
